---
title: "Sentiment Analysis in R"
author: "Jelena JovanoviÄ‡"
output:
  pdf_document:
    fig_caption: yes
urlcolor: blue
---

# Get the tweets

The data set consists of 844 tweets with #apple hashtag. All tweets are labeles as either positive ('POS') or negative ('NEG'), sentiment wise.
```{r}
# load the tweets data
tweets.data <- read.csv(file = "data/tweets_sentiment_labelled.csv",  
                        colClasses = c("character", "factor"))
str(tweets.data)
```

Examine a few positive and a few negative tweets.
```{r}
tweets.data$Tweet[tweets.data$Lbl=="POS"][1:10]
tweets.data$Tweet[tweets.data$Lbl=="NEG"][1:10]
```

Examine the distribution of class labels.

```{r}
table(tweets.data$Lbl)
```

# Preprocessing of tweets' text

We use the 'tm' package that allows performing text mining related tasks.  

```{r echo=FALSE}
library(tm)
```

We will also load out custom R script with auxiliary functions.
```{r}
source("text_mining_utils.R")
```

In TM terminology, *corpora* are collections of documents containing (natural language) text. In order to work with textual datasets in 'tm', we need to create a 'Corpus' instance
```{r}
# build a corpus
tw.corpus <- Corpus(VectorSource(tweets.data$Tweet))
tw.corpus
```

tm_map() function (from the 'tm' package) allows for performing different transformations on the corpus. List of frequently used transformations can be obtained with the 'getTransformations()' function.
```{r}
getTransformations()
```

The purpose of all the transformations is to reduce the diversity among the words and remove words that are of low importance.


The first transformation will be to convert text to lower case.

```{r}
tw.corpus <- tm_map(tw.corpus, tolower)
print.tweets(tw.corpus, 1, 10)
```

When processing tweets, we often remove user references completely. However, this corpus is specific - it has many (meaningful) user references; those are mostly references to Twitter accounts of various tech companies. So, we will remove only '@' sign that marks usernames (@username). This will be done using regular expressions. An excellent introduction to regular expression is [The Bastards Book of Regular Expressions](http://regex.bastardsbook.com/).

```{r}
replaceUserRefs <- function(x) gsub("@(\\w+)", "\\1", x)
tw.corpus <- tm_map(tw.corpus, replaceUserRefs)
print.tweets( tw.corpus, from = 10, to = 20 )
```

Remove hash (#) sign from hastags.
```{r}
removeHash <- function(x) gsub("#([[:alnum:]]+)", "\\1", x)  
tw.corpus <- tm_map(tw.corpus, removeHash)
print.tweets( tw.corpus, from = 10, to = 20 )
```

Replace URLs with the "URL" term.
```{r}
replaceURL <- function(x) gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", "URL", x) 
tw.corpus <- tm_map(tw.corpus, replaceURL)
print.tweets( tw.corpus, from = 10, to = 20 )
```

Replace links to pictures (e.g. pic.twitter.com/lbu9diufrf) with 'TW_PIC'.

```{r}
replaceTWPic <- function(x) gsub("pic\\.twitter\\.com/[[:alnum:]]+", 
                                 "TW_PIC", x) 
tw.corpus <- tm_map(tw.corpus, replaceTWPic)
print.tweets( tw.corpus, from = 10, to = 20 )
```

Replace :-), :), ;-), :D, :o with the "POS_SMILEY" term.
```{r}
replaceHappySmiley <- function(x) gsub("[:|;](-?)[\\)|o|O|D]", 
                                       "POS_SMILEY", x)
tw.corpus <- tm_map(tw.corpus, replaceHappySmiley)
print.tweets( tw.corpus, from = 10, to = 20 )
```

Replace :(, :-(, :/, >:(, >:O with the "NEG_SMILEY" term.

```{r}
replaceSadSmiley <- function(x) gsub("((>?):(-?)[\\(|/|O|o])", 
                                     "NEG_SMILEY", x)
tw.corpus <- tm_map(tw.corpus, replaceSadSmiley)
```

Remove stopwords from the corpus. First, examine the tm's set of stopwords.

```{r}
stopwords('english')[100:120]
```

Add a few extra ('corpus-specific') stop words (e.g. "apple", "rt") to the 'general' stopwords for the English language.

```{r}
tw.stopwords <- c(stopwords('english'), "apple", "rt")
tw.corpus <- tm_map(tw.corpus, removeWords, tw.stopwords)
print.tweets( tw.corpus, from = 20, to = 30 )
```

Remove punctuation.
```{r}
tw.corpus <- tm_map(tw.corpus, removePunctuation, 
                    preserve_intra_word_contractions = TRUE,
                    preserve_intra_word_dashes = TRUE)
print.tweets( tw.corpus, from = 20, to = 30 )
```

Remove stand-alone numbers (but not numbers in e.g. iphone7 or g3)
```{r}
removeStandAloneNumbers <- function(x) gsub(" \\d+ ", "", x)
tw.corpus <- tm_map(tw.corpus, removeStandAloneNumbers)
print.tweets( tw.corpus, from = 20, to = 30 )
```

Strip whitespace.
```{r}
tw.corpus <- tm_map(tw.corpus, stripWhitespace)
```

Do word stemming using the [Snowball stemmer](https://snowballstem.org/). To use the Snowball stemmer in R, we need the 'SnowballC' package.

```{r echo=FALSE}
#install.packages("SnowballC")
library(SnowballC)
```

Since we might later want to have words in their 'regular' form, we will keep a copy of the corpus before stemming it.

```{r}
tw.corpus.backup <- tw.corpus
```

Now, do the stemming.

```{r}
tw.corpus <- tm_map(tw.corpus, stemDocument, language = "english") 
print.tweets( tw.corpus, from = 20, to = 30)
```

# Building a Document-Term matrix

Document Term Matrix (DTM) represents the relationship between terms and documents, where each row stands for a document and each column for a term, and entry is the weight of the term in the corresponding document.

```{r}
min.freq <- round(0.005*length(tw.corpus))
max.freq <- round(0.95*length(tw.corpus))
dtm <- DocumentTermMatrix(tw.corpus, 
                          control = list(bounds = list(global = c(min.freq,max.freq)),
                                         wordLengths = c(2,16), # the restriction on the word length
                                         weighting = weightTf)) # term freq. weighting scheme
```

**Note:** the 'global' parameter is altered to require a word to appear in at least ~0.5% and at most in 95% of tweets to be included in the matrix. Check the documentation of the *TermDocumentMatrix* function for other useful control parameters.

Let's examine the built DTM matrix.
```{r}
inspect(dtm)
```

We have very sparse DTM matrix; so, we should better reduce the sparsity by removing overly sparse terms.

```{r}
dtm.trimmed <- removeSparseTerms(dtm, sparse = 0.9875)
inspect(dtm.trimmed)
```

Examine the resulting DTM matrix. First, check the terms that appear at least 20 times in the whole corpus.

```{r}
findFreqTerms(dtm.trimmed, lowfreq = 20)
```

We can also inspect the frequency of accurance of all the terms.

```{r}
head(colSums(as.matrix(dtm)))
```

It is better if they are sorted.

```{r}
head(sort(colSums(as.matrix(dtm)), decreasing = T), n = 10)
```

# Classifying tweets using Naive Bayes method

Since we want to use DTM for classification purposes, we need to transform it into a data frame that can be passed to a function for building a classifier.

```{r}
features.final <- as.data.frame(as.matrix(dtm.trimmed))
str(features.final, list.len = 50)
```

Add the class label.

```{r}
features.final$CLASS_LBL <- tweets.data$Lbl 
colnames(features.final)
```

Split the data into training and test sets.

```{r}
library(caret)
set.seed(1212)
train.indices <- createDataPartition(y = features.final$CLASS_LBL,
                                     p = 0.85,
                                     list = FALSE)
train.data <- features.final[train.indices,]
test.data <- features.final[-train.indices,]
```

Build NB classifier using all the features.

```{r echo=FALSE}
#install.packages('e1071')
library(e1071)

nb1 <- naiveBayes(CLASS_LBL ~ ., 
                  data = train.data, 
                  laplace = 1) # laplace smoothing (correction)
```

Since each feature (word) has numerous zero values, when fitting the model, we include the Laplace smoothing to avoid zero values of conditional probabilities. 

Let's make the predictions.

```{r}
nb1.pred <- predict(nb1, newdata = test.data, type = "class")
```

Create confusion matrix.

```{r}
cm1 <- table(true = test.data$CLASS_LBL, predicted = nb1.pred)
cm1
```

Evaluate the model.

```{r}
eval1 <- compute.eval.measures(cm1)
eval1
```

Try to improve the performance by using a different probability threshold (instead of the default one of 0.5). To that end, we'll make use of ROC curves.

```{r echo=FALSE}
#install.packages('pROC')
library(pROC)
```

Get predictions as probabilities.

```{r}
nb1.pred.prob <- predict(nb1, newdata = test.data, type = "raw")
nb1.pred.prob[1:10,]
```

Compute the stats for the ROC curve.

```{r}
nb.roc <- roc(response = as.numeric(test.data$CLASS_LBL),
              predictor = nb1.pred.prob[,1], # probabilities of the 'positive' class
              levels = c(2,1)) # define the order of levels corresponding to the negative (controls)
                              # and positive (cases) class
```

Plot the curve:

```{r}
plot.roc(x = nb.roc,
         print.auc = TRUE) # print AUC measure
```

Get the evaluation measures and the threshold for the local maxima of the ROC curve.

```{r}
nb2.coords <- coords(roc = nb.roc,
                     x = "local maximas",
                     ret = c("accuracy", "sensitivity", "specificity", "thr"))
nb2.coords
```

As we want to assure that the company (Apple) will not miss tweets with negative sentiment, and since we set the negative sentiment as our positive class (i.e. class in our focus), we should look for a probability threshold that will maximize sensitivity (i.e., true positive rate). Still, we should keep the other measures (accuracy, specificity) at a decent level.


The local maximum that corresponds to the 9th column looks like a good candidate. Let's examine it more closely:

```{r}
nb2.coords[,9]
```

Select the threshold that corresponds to the 9th local maximum:
```{r}
opt.threshold <- nb2.coords[4,9]
```

Assign class labels based on the chosen threshold:

```{r}
nb1.pred.opt <- ifelse(test = nb1.pred.prob[,1] > opt.threshold,
                       yes = "NEG", no = "POS")
nb1.pred.opt <- as.factor(nb1.pred.opt)
```

Create a confusion matrix based on the newly assigned class labels:

```{r}
cm.opt <- table(actual = test.data$CLASS_LBL, predicted = nb1.pred.opt)
cm.opt
```

Examine evaluation measures:

```{r}
eval2 <- compute.eval.measures(cm.opt)
eval2
```

Compare evaluation measures:

```{r}
data.frame(rbind(eval1, eval2), row.names = c("default_threshold", "ROC_based_theshold"))
```

# Acknowledgements

This example is partially based on *Chapter 10* of the [R and Data Mining](http://www.rdatamining.com/books/rdm) book.